{"instruction": "Produce the structured JSON note for this paper.", "input": "Title: Fast Sorting. Abstract: We propose FastSort, a sorting algorithm. Methods: We use divide-and-conquer. Results: Beats baseline.", "output": "{\"title\": \"Fast Sorting\", \"authors\": [\"Alice\", \"Bob\"], \"summary\": {\"background\": \"Sorting is a fundamental problem.\", \"claim\": \"FastSort improves sorting speed over baseline.\", \"methods\": \"Divide-and-conquer algorithm with optimized merging.\", \"results\": \"Outperforms baseline by X% on benchmarks.\", \"limitations\": \"Needs more tests on larger datasets.\"}, \"key_quotes\": [\"\\\"We propose FastSort, a sorting algorithm.\\\"\"], \"action_items\": [\"Test on dataset Y\", \"Open-source the code\"], \"references_to_read\": [\"Baseline paper A\"]}"}\n{"instruction": "Produce the structured JSON note for this paper.", "input": "Title: TinyNet. Abstract: TinyNet reduces parameters. Methods: Pruning and distillation. Results: Smaller model with similar accuracy.", "output": "{\"title\": \"TinyNet\", \"authors\": [\"Carol\"], \"summary\": {\"background\": \"Model compression is important for edge devices.\", \"claim\": \"TinyNet achieves strong compression with little accuracy loss.\", \"methods\": \"Pruning followed by knowledge distillation.\", \"results\": \"Comparable accuracy with 10x fewer parameters.\", \"limitations\": \"May not generalize to all tasks.\"}, \"key_quotes\": [\"\\\"TinyNet reduces parameters.\\\"\"], \"action_items\": [\"Evaluate on mobile benchmarks\", \"Compare with other distillation methods\"], \"references_to_read\": [\"DistilBERT paper\"]}"}\n